{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import Env, spaces, utils\n",
    "import pygame\n",
    "from typing import Optional\n",
    "from io import StringIO\n",
    "from contextlib import closing\n",
    "from gym.spaces import Discrete\n",
    "from gym.wrappers import FlattenObservation\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/tensorflow/agents/blob/master/docs/tutorials/9_c51_tutorial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.agents.categorical_dqn import categorical_dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import categorical_q_network, q_network\n",
    "from tf_agents.policies import random_tf_policy, greedy_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env_2048 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import clear_output\n",
    "# env = BlockDoubleEnv()\n",
    "# env.reset()  # reset enviornment to default state\n",
    "# score = 0\n",
    "# while True:\n",
    "#     check = 0\n",
    "#     print(env.render())\n",
    "#     key = input(\"w,a,s,d\\n\").lower()\n",
    "#     if key == \"w\":\n",
    "#         action = 3\n",
    "#     elif key == \"d\":\n",
    "#         action = 0\n",
    "#     elif key == \"s\":\n",
    "#         action = 1\n",
    "#     elif key == \"a\":\n",
    "#         action = 2\n",
    "#     else:\n",
    "#         break\n",
    "#     clear_output()\n",
    "#     new_state, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparamaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 100 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 1000  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_capacity = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "gamma = 0.99\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_atoms = 51  # @param {type:\"integer\"}\n",
    "min_q_value = -20  # @param {type:\"integer\"}\n",
    "max_q_value = 20  # @param {type:\"integer\"}\n",
    "n_step_update = 2  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlockDoubleEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = suite_gym.wrap_env(BlockDoubleEnv())\n",
    "eval_py_env = suite_gym.wrap_env(BlockDoubleEnv())\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Rescaling\n",
    "q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    preprocessing_layers = Rescaling(scale=1./2048),\n",
    "    #num_atoms=num_atoms,\n",
    "    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(16,), dtype=tf.int32, name='observation', minimum=array(0), maximum=array(2048))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env.observation_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.networks import sequential\n",
    "fc_layer_params = (100, )\n",
    "action_tensor_spec = tensor_spec.from_spec(train_env.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "# Define a helper function to create Dense layers configured with the right\n",
    "# activation and kernel initializer.\n",
    "def dense_layer(num_units):\n",
    "  return tf.keras.layers.Dense(\n",
    "      num_units,\n",
    "      activation=tf.keras.activations.relu,\n",
    "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
    "# with `num_actions` units to generate one q_value per available action as\n",
    "# its output.\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "q_values_layer = tf.keras.layers.Dense(\n",
    "    num_actions,\n",
    "    activation=None,\n",
    "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "        minval=-0.03, maxval=0.03),\n",
    "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "\n",
    "from tensorflow.keras.layers import Reshape, Conv2D, Flatten\n",
    "\n",
    "reshape = [Reshape((4, 4), input_shape=(16,))]\n",
    "NUM_FILTERS_LAYER_1 = 32 # number of filters in 1st layer\n",
    "NUM_FILTERS_LAYER_2 = 64 # number of filters in 2nd layer\n",
    "FILTERS_SIZE_LAYER_1 = 3 # Filter Size = 3 x 3\n",
    "FILTERS_SIZE_LAYER_2 = 1 # Filter Size = 1 x 1\n",
    "STRIDES_LAYER_1 = (2, 2)\n",
    "STRIDES_LAYER_2 = (1, 1)\n",
    "ACTIVATION_FTN_CNN = 'relu'\n",
    "# Dense Layers:\n",
    "NUM_DENSE_NEURONS = 512\n",
    "ACTIVATION_FTN_DENSE = 'relu'\n",
    "ACTIVATION_FTN_OUTPUT = 'linear'\n",
    "INPUT_SHAPE_CNN = (1,4,4)  \n",
    "conv_layers = [ Conv2D(filters=NUM_FILTERS_LAYER_1,\n",
    "                    kernel_size=FILTERS_SIZE_LAYER_1,\n",
    "                    strides=STRIDES_LAYER_1,\n",
    "                    padding='valid',\n",
    "                    activation=ACTIVATION_FTN_CNN,\n",
    "                    input_shape=INPUT_SHAPE_CNN), #, data_format=data_format)) \n",
    "Conv2D(filters=NUM_FILTERS_LAYER_2,\n",
    "                    kernel_size=FILTERS_SIZE_LAYER_2,\n",
    "                    strides=STRIDES_LAYER_2,\n",
    "                    padding='valid',\n",
    "                    activation=ACTIVATION_FTN_CNN,\n",
    "                    input_shape=INPUT_SHAPE_CNN)\n",
    "]\n",
    "q_net = sequential.Sequential([Rescaling(scale=1./2048)]  + reshape + conv_layers + [Flatten()] +  dense_layers + [q_values_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.networks import sequential\n",
    "fc_layer_params = (1024, 512, 256)\n",
    "action_tensor_spec = tensor_spec.from_spec(train_env.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "# Define a helper function to create Dense layers configured with the right\n",
    "# activation and kernel initializer.\n",
    "def dense_layer(num_units):\n",
    "  return tf.keras.layers.Dense(\n",
    "      num_units,\n",
    "      activation=tf.keras.activations.relu,\n",
    "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
    "# with `num_actions` units to generate one q_value per available action as\n",
    "# its output.\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "q_values_layer = tf.keras.layers.Dense(\n",
    "    num_actions,\n",
    "    activation=None,\n",
    "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "        minval=-0.03, maxval=0.03),\n",
    "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "\n",
    "from tensorflow.keras.layers import Reshape, Conv2D, Flatten\n",
    "\n",
    "reshape = [Reshape((4, 4 ,1), input_shape=(16,))]\n",
    "\n",
    "conv_layers = [ Conv2D(filters=32, kernel_size=3, input_shape=(1,4,4)), Conv2D(filters=64, kernel_size=1, input_shape=(1,4,4))]\n",
    "q_net = sequential.Sequential([Rescaling(scale=1./2048)] + reshape  + conv_layers + [Flatten()] +  dense_layers + [q_values_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network = q_net,\n",
    "    # categorical_q_network=categorical_q_net,\n",
    "    optimizer=optimizer,\n",
    "    # min_q_value=min_q_value,\n",
    "    # max_q_value=max_q_value,\n",
    "    \n",
    "    n_step_update=n_step_update,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    gamma=gamma,\n",
    "    train_step_counter=train_step_counter)\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "    score = time_step.observation.numpy().sum()\n",
    "  avg_score = score / num_episodes\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0], avg_score\n",
    "\n",
    "def compute_avg_score(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_score = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_score = 0.0\n",
    "\n",
    "    while time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_score += time_step._get_info[0]\n",
    "    total_score += episode_score\n",
    "\n",
    "  avg_return = total_score / num_episodes\n",
    "  return avg_return.numpy()[0]\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())\n",
    "\n",
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)\n",
    "\n",
    "# Please also see t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "tf.Tensor([[0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[0 0 2 0 0 0 0 2 0 0 0 0 0 0 0 0]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[0 0 0 0 0 0 0 0 2 0 0 0 0 0 2 2]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[0 0 0 0 0 0 0 0 0 0 2 2 0 0 0 4]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[2 0 0 0 0 0 0 0 4 0 0 0 4 0 0 0]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[2 0 0 2 0 0 0 0 0 0 0 4 0 0 0 4]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[0 0 0 0 0 0 2 0 0 0 0 2 2 0 0 8]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[2 0 2 2 0 0 0 8 0 0 0 0 0 0 0 2]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[2 0 2 2 0 0 0 8 2 0 0 2 0 0 0 0]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[0 0 2 4 0 0 0 8 2 0 0 4 0 0 0 0]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[2 0 2 4 0 0 0 8 0 0 2 4 0 0 0 0]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[0 0 2 0 0 0 0 4 0 0 0 8 2 0 4 4]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[0 0 0 2 0 0 2 4 0 0 0 8 0 0 2 8]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 0  0  4  2  0  0  0  4  2  0  0 16  0  0  0  0]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 0  0  0  0  0  0  0  2  2  0  0  4  2  0  4 16]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 4  0  4  2  0  0  0  4  0  0  0 16  0  2  0  0]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 2  0  0  0  0  0  0  2  0  0  0  4  4  2  4 16]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 0  0  0  2  0  2  0  2  0  0  0  4  4  2  4 16]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 4  4  4  4  0  0  0  4  0  0  0 16  0  0  0  4]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 0  0  0  0  0  0  0  8  4  0  0 16  4  4  4  4]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 8  4  4  8  0  4  0 16  0  0  0  4  0  0  0  0]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 2  0  0  0  0  0  0  8  0  0  0 16  8  8  4  4]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 0  0  2  2  0  0  0  8  0  0  0 16  0  0 16  8]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 0  0  2  2  0  0 16  8  0  2  0 16  0  0  0  8]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 4  0  0  0 16  8  0  0  2 16  0  2  8  0  0  0]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 0  0  2  4  0  0 16  8  0  2 16  2  0  0  0  8]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 2  4  0  2 16  8  0  0  2 16  2  0  8  0  0  0]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 0  2  4  2  0  0 16  8  0  2 16  2  0  0  2  8]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 0  0  0  2  0  2  4  8  0  0 32  2  0  4  2  8]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 2  0  0  0  2  4  8  0 32  2  2  0  4  2  8  0]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 0  0  0  2  2  2  4  8  0  0 32  4  0  4  2  8]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 2  0  2  0  4  4  8  0 32  4  0  0  4  2  8  0]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 2  0  0  4  0  0  8  8  0  0 32  4  0  4  2  8]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 2  4  0  0 16  2  0  0 32  4  0  0  4  2  8  0]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 2  4  8  0 16  2  2  0 32  4  0  0  4  2  0  0]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 0  2  4  8  0  0 16  4  0  0 32  4  2  0  4  2]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 0  2  4  0  0  0 16  8  0  0 32  8  2  2  4  2]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 2  4  0  0 16  8  0  0 32  8  0  0  4  4  2  2]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 2  4  2  2 16 16  0  0 32  4  2  0  4  0  0  0]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 0  2  4  4  0  0  2 32  0 32  4  2  0  0  0  4]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 0  0  2  4  0  0  4 32  0  2  2  2  0 32  4  4]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 0  0  2  4  0  0  4 32  0  2  2  4  0  0 32  8]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 0  2  2  4  0  0  4 32  2  0  2  4  0  0 32  8]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 0  0  4  4  0  0  4 32  2  0  4  4  0  0 32  8]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 2  0  8  4  0  2  4 32  0  0 32  4  0  0  0  8]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 2  8  4  2  2  4 32  0 32  4  0  0  8  0  0  0]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 2  8  4  2  0  2  4 32  0  0 32  4  0  0  2  8]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 0  0  2  2  0  0  8 32  0  8 32  4  2  2  2  8]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 4  2  0  0  8 32  0  0  8 32  4  0  4  2  8  0]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 4  2  4  0 16 64  8  0  4  2  0  0  0  0  2  0]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 0  4  0  0  4  2  4  0 16 64  8  0  4  2  2  0]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 2  0  0  4  0  4  2  4  0 16 64  8  0  0  4  4]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 2  4  0  2  4  2  4  0 16 64  8  0  8  0  0  0]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 2  2  4  2  0  4  2  4  0 16 64  8  0  0  0  8]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 4  4  2  2  4  2  4  0 16 64  8  0  8  0  0  0]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 0  4  0  0  8  4  2  0 16  2  4  0  8 64  8  2]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 8  8  2  2 16  2  4  0  8 64  8  0  0  0  0  2]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 2  0  0  0  8  8  2  0 16  2  4  0  8 64  8  4]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 2  8  2  4  8  2  4  0 16 64  8  2  8  0  0  0]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 2  4  0  0  8  8  2  0 16  2  4  4  8 64  8  2]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[ 2  4  2  4  8  8  4  2 16  2  8  0  8 64  2  0]], shape=(1, 16), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "292.4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment = eval_env\n",
    "policy = agent.policy \n",
    "\n",
    "total_return = 0.0\n",
    "\n",
    "time_step = environment.reset()\n",
    "episode_return = 0.0\n",
    "print(\"START\")\n",
    "while not time_step.is_last():\n",
    "  print(time_step.observation)\n",
    "  action_step = policy.action(time_step)\n",
    "  time_step = environment.step(action_step.action)\n",
    "  episode_return += time_step.reward\n",
    "total_return += episode_return\n",
    "\n",
    "avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_avg_return(eval_env, agent.policy, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ellio\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_capacity)\n",
    "\n",
    "def collect_step(environment, policy):\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "  # Add trajectory to the replay buffer\n",
    "  replay_buffer.add_batch(traj)\n",
    "\n",
    "for _ in range(initial_collect_steps):\n",
    "  collect_step(train_env, random_policy)\n",
    "\n",
    "# This loop is so common in RL, that we provide standard implementations of\n",
    "# these. For more details see the drivers module.\n",
    "\n",
    "# Dataset generates trajectories with shape [BxTx...] where\n",
    "# T = n_step_update + 1.\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, sample_batch_size=batch_size,\n",
    "    num_steps=n_step_update + 1).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 10000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 1000  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_capacity = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "gamma = 0.99\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_atoms = 51  # @param {type:\"integer\"}\n",
    "min_q_value = -20  # @param {type:\"integer\"}\n",
    "max_q_value = 20  # @param {type:\"integer\"}\n",
    "n_step_update = 2  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ellio\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
      "step = 200: loss = 242.02471923828125\n",
      "step = 400: loss = 477.23846435546875\n",
      "step = 600: loss = 257.37615966796875\n",
      "step = 800: loss = 2576.6806640625\n",
      "step = 1000: loss = 1498.20849609375\n",
      "step = 1000: Average Return = 6.40\n",
      "step = 1200: loss = 514.2685546875\n",
      "step = 1400: loss = 346.1334228515625\n",
      "step = 1600: loss = 703.2884521484375\n",
      "step = 1800: loss = 530.3494873046875\n",
      "step = 2000: loss = 361.6815490722656\n",
      "step = 2000: Average Return = 1.20\n",
      "step = 2200: loss = 597.1474609375\n",
      "step = 2400: loss = 814.6648559570312\n",
      "step = 2600: loss = 1417.0101318359375\n",
      "step = 2800: loss = 2719.98876953125\n",
      "step = 3000: loss = 1820.596435546875\n",
      "step = 3000: Average Return = 74.00\n",
      "step = 3200: loss = 2099.7900390625\n",
      "step = 3400: loss = 1651.3359375\n",
      "step = 3600: loss = 1061.49658203125\n",
      "step = 3800: loss = 1827.5831298828125\n",
      "step = 4000: loss = 1705.78466796875\n",
      "step = 4000: Average Return = 154.00\n",
      "step = 4200: loss = 1725.9478759765625\n",
      "step = 4400: loss = 1687.6396484375\n",
      "step = 4600: loss = 1930.108154296875\n",
      "step = 4800: loss = 2606.160400390625\n",
      "step = 5000: loss = 1087.7374267578125\n",
      "step = 5000: Average Return = 56.00\n",
      "step = 5200: loss = 1101.4566650390625\n",
      "step = 5400: loss = 1100.669189453125\n",
      "step = 5600: loss = 949.98876953125\n",
      "step = 5800: loss = 2690.396484375\n",
      "step = 6000: loss = 1953.2265625\n",
      "step = 6000: Average Return = 177.60\n",
      "step = 6200: loss = 1006.8380737304688\n",
      "step = 6400: loss = 3431.626953125\n",
      "step = 6600: loss = 1065.1763916015625\n",
      "step = 6800: loss = 2256.4443359375\n",
      "step = 7000: loss = 4327.7255859375\n",
      "step = 7000: Average Return = 136.00\n",
      "step = 7200: loss = 2375.17529296875\n",
      "step = 7400: loss = 880.6246948242188\n",
      "step = 7600: loss = 2708.9736328125\n",
      "step = 7800: loss = 3608.05615234375\n",
      "step = 8000: loss = 3277.474609375\n",
      "step = 8000: Average Return = 208.00\n",
      "step = 8200: loss = 1199.3350830078125\n",
      "step = 8400: loss = 1580.820068359375\n",
      "step = 8600: loss = 1479.668212890625\n",
      "step = 8800: loss = 3045.902587890625\n",
      "step = 9000: loss = 1185.698486328125\n",
      "step = 9000: Average Return = 213.20\n",
      "step = 9200: loss = 2206.70556640625\n",
      "step = 9400: loss = 3129.032470703125\n",
      "step = 9600: loss = 1288.1064453125\n",
      "step = 9800: loss = 4308.4921875\n",
      "step = 10000: loss = 3712.126953125\n",
      "step = 10000: Average Return = 173.60\n"
     ]
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "try:\n",
    "  %%time\n",
    "except:\n",
    "  pass\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return, avg_score = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "for _ in range(num_iterations):\n",
    "  #print(_)\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  for _ in range(collect_steps_per_iteration):\n",
    "    #print(_)\n",
    "    collect_step(train_env, agent.collect_policy)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience)\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return, avg_score = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1:.2f} Average Score = {1:.2f}'.format(step, avg_return, avg_score))\n",
    "    returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 40200: loss = 2581.43408203125\n",
      "step = 40400: loss = 2451.46337890625\n",
      "step = 40600: loss = 1150.9716796875\n",
      "step = 40800: loss = 2123.978515625\n",
      "step = 41000: loss = 1275.426513671875\n",
      "step = 41000: Average Return = 270.80\n",
      "step = 41200: loss = 1845.7103271484375\n",
      "step = 41400: loss = 2367.851806640625\n",
      "step = 41600: loss = 2475.6123046875\n",
      "step = 41800: loss = 2100.190673828125\n",
      "step = 42000: loss = 1361.25927734375\n",
      "step = 42000: Average Return = 335.20\n",
      "step = 42200: loss = 799.846923828125\n",
      "step = 42400: loss = 3115.971923828125\n",
      "step = 42600: loss = 3633.2626953125\n",
      "step = 42800: loss = 3144.14111328125\n",
      "step = 43000: loss = 1674.4761962890625\n",
      "step = 43000: Average Return = 241.20\n",
      "step = 43200: loss = 2088.58935546875\n",
      "step = 43400: loss = 1814.13525390625\n",
      "step = 43600: loss = 2128.239501953125\n",
      "step = 43800: loss = 2347.887451171875\n",
      "step = 44000: loss = 2365.337158203125\n",
      "step = 44000: Average Return = 289.20\n",
      "step = 44200: loss = 6664.958984375\n",
      "step = 44400: loss = 2335.933349609375\n",
      "step = 44600: loss = 3986.229736328125\n",
      "step = 44800: loss = 1850.7967529296875\n",
      "step = 45000: loss = 2424.89453125\n",
      "step = 45000: Average Return = 351.60\n",
      "step = 45200: loss = 1263.66748046875\n",
      "step = 45400: loss = 2270.7392578125\n",
      "step = 45600: loss = 2445.03125\n",
      "step = 45800: loss = 1048.4197998046875\n",
      "step = 46000: loss = 1473.7322998046875\n",
      "step = 46000: Average Return = 362.00\n",
      "step = 46200: loss = 2336.838623046875\n",
      "step = 46400: loss = 4260.23046875\n",
      "step = 46600: loss = 2840.087890625\n",
      "step = 46800: loss = 1835.1136474609375\n",
      "step = 47000: loss = 2335.12646484375\n",
      "step = 47000: Average Return = 245.60\n",
      "step = 47200: loss = 3644.147705078125\n",
      "step = 47400: loss = 3416.493896484375\n",
      "step = 47600: loss = 1908.8427734375\n",
      "step = 47800: loss = 2683.85888671875\n",
      "step = 48000: loss = 1673.494140625\n",
      "step = 48000: Average Return = 246.80\n",
      "step = 48200: loss = 2297.71826171875\n",
      "step = 48400: loss = 1821.8292236328125\n",
      "step = 48600: loss = 1994.8760986328125\n",
      "step = 48800: loss = 2793.93798828125\n",
      "step = 49000: loss = 2088.814453125\n",
      "step = 49000: Average Return = 294.40\n",
      "step = 49200: loss = 1552.21240234375\n",
      "step = 49400: loss = 2003.8499755859375\n",
      "step = 49600: loss = 4031.3935546875\n",
      "step = 49800: loss = 1788.286376953125\n",
      "step = 50000: loss = 2352.27587890625\n",
      "step = 50000: Average Return = 311.60\n",
      "step = 50200: loss = 1461.8194580078125\n",
      "step = 50400: loss = 1707.895751953125\n",
      "step = 50600: loss = 3668.105712890625\n",
      "step = 50800: loss = 1673.065185546875\n",
      "step = 51000: loss = 2713.504638671875\n",
      "step = 51000: Average Return = 275.20\n",
      "step = 51200: loss = 2394.4072265625\n",
      "step = 51400: loss = 5037.55908203125\n",
      "step = 51600: loss = 1171.04345703125\n",
      "step = 51800: loss = 1178.697021484375\n",
      "step = 52000: loss = 2064.24365234375\n",
      "step = 52000: Average Return = 336.40\n",
      "step = 52200: loss = 853.2034912109375\n",
      "step = 52400: loss = 883.7593994140625\n",
      "step = 52600: loss = 2381.44384765625\n",
      "step = 52800: loss = 2243.67333984375\n",
      "step = 53000: loss = 1231.0142822265625\n",
      "step = 53000: Average Return = 358.00\n",
      "step = 53200: loss = 3105.005615234375\n",
      "step = 53400: loss = 3484.27880859375\n",
      "step = 53600: loss = 2370.5771484375\n",
      "step = 53800: loss = 1337.3531494140625\n",
      "step = 54000: loss = 4585.02294921875\n",
      "step = 54000: Average Return = 273.20\n",
      "step = 54200: loss = 3777.21240234375\n",
      "step = 54400: loss = 3566.41748046875\n",
      "step = 54600: loss = 2507.136962890625\n",
      "step = 54800: loss = 2497.490478515625\n",
      "step = 55000: loss = 2311.91943359375\n",
      "step = 55000: Average Return = 262.00\n",
      "step = 55200: loss = 4475.1796875\n",
      "step = 55400: loss = 1346.1224365234375\n",
      "step = 55600: loss = 3910.85205078125\n",
      "step = 55800: loss = 2715.6669921875\n",
      "step = 56000: loss = 2042.9981689453125\n",
      "step = 56000: Average Return = 269.20\n",
      "step = 56200: loss = 1879.6512451171875\n",
      "step = 56400: loss = 687.8280029296875\n",
      "step = 56600: loss = 1686.0738525390625\n",
      "step = 56800: loss = 2041.5830078125\n",
      "step = 57000: loss = 1808.35107421875\n",
      "step = 57000: Average Return = 392.00\n",
      "step = 57200: loss = 1922.5579833984375\n",
      "step = 57400: loss = 2523.9853515625\n",
      "step = 57600: loss = 2563.3154296875\n",
      "step = 57800: loss = 2272.28564453125\n",
      "step = 58000: loss = 2452.06689453125\n",
      "step = 58000: Average Return = 310.40\n",
      "step = 58200: loss = 1606.966552734375\n",
      "step = 58400: loss = 2659.816162109375\n",
      "step = 58600: loss = 2821.041748046875\n",
      "step = 58800: loss = 1752.2802734375\n",
      "step = 59000: loss = 1467.72900390625\n",
      "step = 59000: Average Return = 331.20\n",
      "step = 59200: loss = 1131.114990234375\n",
      "step = 59400: loss = 3049.73291015625\n",
      "step = 59600: loss = 2328.033203125\n",
      "step = 59800: loss = 1212.939697265625\n",
      "step = 60000: loss = 1854.662109375\n",
      "step = 60000: Average Return = 175.60\n",
      "step = 60200: loss = 1698.9732666015625\n",
      "step = 60400: loss = 2393.23095703125\n",
      "step = 60600: loss = 3695.560546875\n",
      "step = 60800: loss = 5133.02880859375\n",
      "step = 61000: loss = 2121.77197265625\n",
      "step = 61000: Average Return = 266.00\n",
      "step = 61200: loss = 3383.25146484375\n",
      "step = 61400: loss = 2333.109375\n",
      "step = 61600: loss = 1609.444580078125\n",
      "step = 61800: loss = 2431.740478515625\n",
      "step = 62000: loss = 3466.203125\n",
      "step = 62000: Average Return = 296.00\n",
      "step = 62200: loss = 4924.6416015625\n",
      "step = 62400: loss = 1479.32373046875\n",
      "step = 62600: loss = 3246.87109375\n",
      "step = 62800: loss = 1635.558837890625\n",
      "step = 63000: loss = 3155.47900390625\n",
      "step = 63000: Average Return = 332.40\n",
      "step = 63200: loss = 984.9124755859375\n",
      "step = 63400: loss = 2562.15087890625\n",
      "step = 63600: loss = 3171.9951171875\n",
      "step = 63800: loss = 2376.6083984375\n",
      "step = 64000: loss = 1056.98388671875\n",
      "step = 64000: Average Return = 350.00\n",
      "step = 64200: loss = 2309.79150390625\n",
      "step = 64400: loss = 4032.617919921875\n",
      "step = 64600: loss = 2001.8056640625\n",
      "step = 64800: loss = 3452.501953125\n",
      "step = 65000: loss = 4806.515625\n",
      "step = 65000: Average Return = 228.00\n",
      "step = 65200: loss = 1831.8509521484375\n",
      "step = 65400: loss = 3313.100341796875\n",
      "step = 65600: loss = 3559.52392578125\n",
      "step = 65800: loss = 2527.715576171875\n",
      "step = 66000: loss = 5313.0732421875\n",
      "step = 66000: Average Return = 308.40\n",
      "step = 66200: loss = 1078.84130859375\n",
      "step = 66400: loss = 2921.56298828125\n",
      "step = 66600: loss = 2860.255859375\n",
      "step = 66800: loss = 2540.387451171875\n",
      "step = 67000: loss = 3741.25537109375\n",
      "step = 67000: Average Return = 400.40\n",
      "step = 67200: loss = 2887.3447265625\n",
      "step = 67400: loss = 2482.22607421875\n",
      "step = 67600: loss = 2060.282958984375\n",
      "step = 67800: loss = 2266.1689453125\n",
      "step = 68000: loss = 2204.11767578125\n",
      "step = 68000: Average Return = 400.80\n",
      "step = 68200: loss = 2284.422607421875\n",
      "step = 68400: loss = 1496.10791015625\n",
      "step = 68600: loss = 4123.7861328125\n",
      "step = 68800: loss = 3012.7109375\n",
      "step = 69000: loss = 3429.158203125\n",
      "step = 69000: Average Return = 393.20\n",
      "step = 69200: loss = 5848.232421875\n",
      "step = 69400: loss = 1508.2259521484375\n",
      "step = 69600: loss = 2033.3466796875\n",
      "step = 69800: loss = 4707.322265625\n",
      "step = 70000: loss = 1902.408935546875\n",
      "step = 70000: Average Return = 243.20\n",
      "step = 70200: loss = 7911.75927734375\n",
      "step = 70400: loss = 1967.609619140625\n",
      "step = 70600: loss = 2714.54150390625\n",
      "step = 70800: loss = 3282.14208984375\n",
      "step = 71000: loss = 2674.54736328125\n",
      "step = 71000: Average Return = 242.00\n",
      "step = 71200: loss = 2179.57666015625\n",
      "step = 71400: loss = 1089.79345703125\n",
      "step = 71600: loss = 3204.162841796875\n",
      "step = 71800: loss = 3240.14599609375\n",
      "step = 72000: loss = 711.3633422851562\n",
      "step = 72000: Average Return = 338.80\n",
      "step = 72200: loss = 3960.55419921875\n",
      "step = 72400: loss = 1728.4521484375\n",
      "step = 72600: loss = 6687.822265625\n",
      "step = 72800: loss = 2145.3779296875\n",
      "step = 73000: loss = 2054.94921875\n",
      "step = 73000: Average Return = 308.40\n",
      "step = 73200: loss = 3819.2578125\n",
      "step = 73400: loss = 2624.696533203125\n",
      "step = 73600: loss = 1250.68408203125\n",
      "step = 73800: loss = 1653.359375\n",
      "step = 74000: loss = 1606.1876220703125\n",
      "step = 74000: Average Return = 222.80\n",
      "step = 74200: loss = 6034.83837890625\n",
      "step = 74400: loss = 7008.197265625\n",
      "step = 74600: loss = 4568.99755859375\n",
      "step = 74800: loss = 4947.96923828125\n",
      "step = 75000: loss = 3271.228515625\n",
      "step = 75000: Average Return = 229.60\n",
      "step = 75200: loss = 1736.26904296875\n",
      "step = 75400: loss = 4093.494384765625\n",
      "step = 75600: loss = 3315.6376953125\n",
      "step = 75800: loss = 2833.044189453125\n",
      "step = 76000: loss = 3608.866943359375\n",
      "step = 76000: Average Return = 262.40\n",
      "step = 76200: loss = 5113.22998046875\n",
      "step = 76400: loss = 4018.2265625\n",
      "step = 76600: loss = 4742.16650390625\n",
      "step = 76800: loss = 8197.576171875\n",
      "step = 77000: loss = 1180.0384521484375\n",
      "step = 77000: Average Return = 270.00\n",
      "step = 77200: loss = 3482.53759765625\n",
      "step = 77400: loss = 5230.2568359375\n",
      "step = 77600: loss = 2836.85302734375\n",
      "step = 77800: loss = 5785.54296875\n",
      "step = 78000: loss = 2233.758056640625\n",
      "step = 78000: Average Return = 459.60\n",
      "step = 78200: loss = 2859.609375\n",
      "step = 78400: loss = 3029.7646484375\n",
      "step = 78600: loss = 5729.71923828125\n",
      "step = 78800: loss = 3218.341796875\n",
      "step = 79000: loss = 2585.2568359375\n",
      "step = 79000: Average Return = 222.80\n",
      "step = 79200: loss = 1442.838134765625\n",
      "step = 79400: loss = 2872.3662109375\n",
      "step = 79600: loss = 3847.1201171875\n",
      "step = 79800: loss = 2772.674072265625\n",
      "step = 80000: loss = 2622.3818359375\n",
      "step = 80000: Average Return = 290.80\n",
      "step = 80200: loss = 3550.80517578125\n",
      "step = 80400: loss = 2101.2275390625\n",
      "step = 80600: loss = 5939.654296875\n",
      "step = 80800: loss = 1319.087646484375\n",
      "step = 81000: loss = 3434.736328125\n",
      "step = 81000: Average Return = 316.00\n",
      "step = 81200: loss = 5049.9658203125\n",
      "step = 81400: loss = 3139.62451171875\n",
      "step = 81600: loss = 6143.958984375\n",
      "step = 81800: loss = 1147.7646484375\n",
      "step = 82000: loss = 5408.1064453125\n",
      "step = 82000: Average Return = 334.80\n",
      "step = 82200: loss = 6559.15625\n",
      "step = 82400: loss = 6905.763671875\n",
      "step = 82600: loss = 3575.25146484375\n",
      "step = 82800: loss = 2308.23583984375\n",
      "step = 83000: loss = 2087.365234375\n",
      "step = 83000: Average Return = 238.40\n",
      "step = 83200: loss = 8277.1650390625\n",
      "step = 83400: loss = 2834.14892578125\n",
      "step = 83600: loss = 2556.69091796875\n",
      "step = 83800: loss = 594.2196044921875\n",
      "step = 84000: loss = 3761.8486328125\n",
      "step = 84000: Average Return = 241.60\n",
      "step = 84200: loss = 5045.74853515625\n",
      "step = 84400: loss = 2444.7724609375\n",
      "step = 84600: loss = 2908.585205078125\n",
      "step = 84800: loss = 2943.0068359375\n",
      "step = 85000: loss = 2037.8753662109375\n",
      "step = 85000: Average Return = 333.20\n",
      "step = 85200: loss = 1575.557373046875\n",
      "step = 85400: loss = 4653.84033203125\n",
      "step = 85600: loss = 1780.827880859375\n",
      "step = 85800: loss = 3034.61376953125\n",
      "step = 86000: loss = 643.7714233398438\n",
      "step = 86000: Average Return = 300.00\n",
      "step = 86200: loss = 4393.54052734375\n",
      "step = 86400: loss = 1571.31591796875\n",
      "step = 86600: loss = 3309.267578125\n",
      "step = 86800: loss = 1666.726806640625\n",
      "step = 87000: loss = 1508.349365234375\n",
      "step = 87000: Average Return = 162.40\n",
      "step = 87200: loss = 2215.15234375\n",
      "step = 87400: loss = 3302.2265625\n",
      "step = 87600: loss = 2491.3740234375\n",
      "step = 87800: loss = 5522.60400390625\n",
      "step = 88000: loss = 5370.646484375\n",
      "step = 88000: Average Return = 182.40\n",
      "step = 88200: loss = 6934.9951171875\n",
      "step = 88400: loss = 1639.37109375\n",
      "step = 88600: loss = 4017.87646484375\n",
      "step = 88800: loss = 7894.1396484375\n",
      "step = 89000: loss = 4024.197021484375\n",
      "step = 89000: Average Return = 291.20\n",
      "step = 89200: loss = 4007.04150390625\n",
      "step = 89400: loss = 1773.922119140625\n",
      "step = 89600: loss = 5796.3193359375\n",
      "step = 89800: loss = 8061.7890625\n",
      "step = 90000: loss = 1306.0430908203125\n",
      "step = 90000: Average Return = 212.00\n",
      "step = 90200: loss = 1877.77294921875\n",
      "step = 90400: loss = 1893.5841064453125\n",
      "step = 90600: loss = 2381.721923828125\n",
      "step = 90800: loss = 4935.033203125\n",
      "step = 91000: loss = 1232.5224609375\n",
      "step = 91000: Average Return = 276.00\n",
      "step = 91200: loss = 1736.5615234375\n",
      "step = 91400: loss = 7486.77685546875\n",
      "step = 91600: loss = 2006.55712890625\n",
      "step = 91800: loss = 8069.341796875\n",
      "step = 92000: loss = 3801.29931640625\n",
      "step = 92000: Average Return = 325.60\n",
      "step = 92200: loss = 3000.978271484375\n",
      "step = 92400: loss = 6129.6630859375\n",
      "step = 92600: loss = 2245.8671875\n",
      "step = 92800: loss = 2795.371337890625\n",
      "step = 93000: loss = 3459.7626953125\n",
      "step = 93000: Average Return = 290.40\n",
      "step = 93200: loss = 2198.34716796875\n",
      "step = 93400: loss = 4092.091552734375\n",
      "step = 93600: loss = 4495.99365234375\n",
      "step = 93800: loss = 2823.185791015625\n",
      "step = 94000: loss = 6416.5341796875\n",
      "step = 94000: Average Return = 284.80\n",
      "step = 94200: loss = 5322.2109375\n",
      "step = 94400: loss = 3551.24365234375\n",
      "step = 94600: loss = 1704.0982666015625\n",
      "step = 94800: loss = 2190.8828125\n",
      "step = 95000: loss = 5694.67578125\n",
      "step = 95000: Average Return = 306.00\n",
      "step = 95200: loss = 4808.265625\n",
      "step = 95400: loss = 4859.9345703125\n",
      "step = 95600: loss = 7750.9736328125\n",
      "step = 95800: loss = 4033.157470703125\n",
      "step = 96000: loss = 4412.8271484375\n",
      "step = 96000: Average Return = 308.80\n",
      "step = 96200: loss = 4330.8828125\n",
      "step = 96400: loss = 3326.44287109375\n",
      "step = 96600: loss = 2149.9677734375\n",
      "step = 96800: loss = 8720.91015625\n",
      "step = 97000: loss = 1850.809326171875\n",
      "step = 97000: Average Return = 296.40\n",
      "step = 97200: loss = 4538.775390625\n",
      "step = 97400: loss = 4258.28955078125\n",
      "step = 97600: loss = 3492.689453125\n",
      "step = 97800: loss = 5867.45458984375\n",
      "step = 98000: loss = 2655.397216796875\n",
      "step = 98000: Average Return = 329.20\n",
      "step = 98200: loss = 1243.7789306640625\n",
      "step = 98400: loss = 3999.44091796875\n",
      "step = 98600: loss = 5161.7509765625\n",
      "step = 98800: loss = 2720.14013671875\n",
      "step = 99000: loss = 3057.7490234375\n",
      "step = 99000: Average Return = 232.40\n",
      "step = 99200: loss = 5559.5478515625\n",
      "step = 99400: loss = 3803.192138671875\n",
      "step = 99600: loss = 1469.8466796875\n",
      "step = 99800: loss = 4377.02099609375\n",
      "step = 100000: loss = 1499.75439453125\n",
      "step = 100000: Average Return = 290.80\n",
      "step = 100200: loss = 3681.112060546875\n",
      "step = 100400: loss = 6335.1474609375\n",
      "step = 100600: loss = 3972.5859375\n",
      "step = 100800: loss = 2214.813720703125\n",
      "step = 101000: loss = 1732.27392578125\n",
      "step = 101000: Average Return = 228.00\n",
      "step = 101200: loss = 1526.1202392578125\n",
      "step = 101400: loss = 1483.5419921875\n",
      "step = 101600: loss = 2995.99267578125\n",
      "step = 101800: loss = 3668.141357421875\n",
      "step = 102000: loss = 2347.26513671875\n",
      "step = 102000: Average Return = 328.00\n",
      "step = 102200: loss = 1690.37451171875\n",
      "step = 102400: loss = 5660.17138671875\n",
      "step = 102600: loss = 3153.983154296875\n",
      "step = 102800: loss = 717.9557495117188\n",
      "step = 103000: loss = 7945.775390625\n",
      "step = 103000: Average Return = 210.00\n",
      "step = 103200: loss = 6457.19970703125\n",
      "step = 103400: loss = 1371.22119140625\n",
      "step = 103600: loss = 2365.553466796875\n",
      "step = 103800: loss = 4958.4150390625\n",
      "step = 104000: loss = 2523.5791015625\n",
      "step = 104000: Average Return = 324.00\n",
      "step = 104200: loss = 4142.435546875\n",
      "step = 104400: loss = 3029.3759765625\n",
      "step = 104600: loss = 3634.6357421875\n",
      "step = 104800: loss = 4629.38525390625\n",
      "step = 105000: loss = 3271.414306640625\n",
      "step = 105000: Average Return = 405.60\n",
      "step = 105200: loss = 2291.1201171875\n",
      "step = 105400: loss = 2160.162109375\n",
      "step = 105600: loss = 3221.7421875\n",
      "step = 105800: loss = 2095.1796875\n",
      "step = 106000: loss = 6506.3984375\n",
      "step = 106000: Average Return = 171.60\n",
      "step = 106200: loss = 6157.232421875\n",
      "step = 106400: loss = 2800.54736328125\n",
      "step = 106600: loss = 1383.90869140625\n",
      "step = 106800: loss = 3859.41650390625\n",
      "step = 107000: loss = 2976.693359375\n",
      "step = 107000: Average Return = 426.00\n",
      "step = 107200: loss = 4187.8056640625\n",
      "step = 107400: loss = 1390.7471923828125\n",
      "step = 107600: loss = 4542.369140625\n",
      "step = 107800: loss = 1983.6402587890625\n",
      "step = 108000: loss = 5038.544921875\n",
      "step = 108000: Average Return = 346.00\n",
      "step = 108200: loss = 2069.265869140625\n",
      "step = 108400: loss = 5335.8173828125\n",
      "step = 108600: loss = 6215.1904296875\n",
      "step = 108800: loss = 1829.058349609375\n",
      "step = 109000: loss = 3719.90380859375\n",
      "step = 109000: Average Return = 364.00\n",
      "step = 109200: loss = 2206.775634765625\n",
      "step = 109400: loss = 7456.39404296875\n",
      "step = 109600: loss = 8218.861328125\n",
      "step = 109800: loss = 2440.546875\n",
      "step = 110000: loss = 4574.73046875\n",
      "step = 110000: Average Return = 266.40\n",
      "step = 110200: loss = 2295.46484375\n",
      "step = 110400: loss = 1792.3243408203125\n",
      "step = 110600: loss = 3203.288330078125\n",
      "step = 110800: loss = 893.8294677734375\n",
      "step = 111000: loss = 4318.89453125\n",
      "step = 111000: Average Return = 238.40\n",
      "step = 111200: loss = 4148.353515625\n",
      "step = 111400: loss = 2193.2099609375\n",
      "step = 111600: loss = 675.5106201171875\n",
      "step = 111800: loss = 3178.86767578125\n",
      "step = 112000: loss = 6653.33642578125\n",
      "step = 112000: Average Return = 408.00\n",
      "step = 112200: loss = 3900.32177734375\n",
      "step = 112400: loss = 3675.8388671875\n",
      "step = 112600: loss = 4712.9287109375\n",
      "step = 112800: loss = 2736.63427734375\n",
      "step = 113000: loss = 6673.77392578125\n",
      "step = 113000: Average Return = 273.20\n",
      "step = 113200: loss = 1129.7391357421875\n",
      "step = 113400: loss = 4175.97119140625\n",
      "step = 113600: loss = 4940.029296875\n",
      "step = 113800: loss = 3871.83984375\n",
      "step = 114000: loss = 2093.565185546875\n",
      "step = 114000: Average Return = 363.20\n",
      "step = 114200: loss = 4166.3017578125\n",
      "step = 114400: loss = 2360.356689453125\n",
      "step = 114600: loss = 8969.146484375\n",
      "step = 114800: loss = 4722.66015625\n",
      "step = 115000: loss = 3523.702880859375\n",
      "step = 115000: Average Return = 255.60\n",
      "step = 115200: loss = 1645.378662109375\n",
      "step = 115400: loss = 3271.366943359375\n",
      "step = 115600: loss = 3438.175048828125\n",
      "step = 115800: loss = 9262.08984375\n",
      "step = 116000: loss = 5732.11767578125\n",
      "step = 116000: Average Return = 174.40\n",
      "step = 116200: loss = 3416.668212890625\n",
      "step = 116400: loss = 3891.28125\n",
      "step = 116600: loss = 4472.625\n",
      "step = 116800: loss = 5432.197265625\n",
      "step = 117000: loss = 5060.953125\n",
      "step = 117000: Average Return = 337.20\n",
      "step = 117200: loss = 3217.984619140625\n",
      "step = 117400: loss = 3968.150390625\n",
      "step = 117600: loss = 2007.1580810546875\n",
      "step = 117800: loss = 3818.33544921875\n",
      "step = 118000: loss = 2505.663330078125\n",
      "step = 118000: Average Return = 217.20\n",
      "step = 118200: loss = 4235.732421875\n",
      "step = 118400: loss = 8519.08984375\n",
      "step = 118600: loss = 2210.378173828125\n",
      "step = 118800: loss = 1034.8629150390625\n",
      "step = 119000: loss = 4057.2841796875\n",
      "step = 119000: Average Return = 282.80\n",
      "step = 119200: loss = 7195.421875\n",
      "step = 119400: loss = 4220.865234375\n",
      "step = 119600: loss = 4944.16796875\n",
      "step = 119800: loss = 6069.9814453125\n",
      "step = 120000: loss = 5792.8388671875\n",
      "step = 120000: Average Return = 241.20\n",
      "step = 120200: loss = 5154.111328125\n",
      "step = 120400: loss = 6383.28515625\n",
      "step = 120600: loss = 1829.5169677734375\n",
      "step = 120800: loss = 3351.20849609375\n",
      "step = 121000: loss = 6091.21875\n",
      "step = 121000: Average Return = 208.00\n",
      "step = 121200: loss = 2600.283935546875\n",
      "step = 121400: loss = 2542.11328125\n",
      "step = 121600: loss = 2539.162841796875\n",
      "step = 121800: loss = 6865.1552734375\n",
      "step = 122000: loss = 2267.65869140625\n",
      "step = 122000: Average Return = 338.80\n",
      "step = 122200: loss = 4834.6767578125\n",
      "step = 122400: loss = 1066.635986328125\n",
      "step = 122600: loss = 5377.4638671875\n",
      "step = 122800: loss = 6556.44482421875\n",
      "step = 123000: loss = 5052.0146484375\n",
      "step = 123000: Average Return = 270.00\n",
      "step = 123200: loss = 3780.252685546875\n",
      "step = 123400: loss = 8625.34375\n",
      "step = 123600: loss = 2918.04150390625\n",
      "step = 123800: loss = 3730.06640625\n",
      "step = 124000: loss = 1204.6580810546875\n",
      "step = 124000: Average Return = 286.00\n",
      "step = 124200: loss = 4016.365966796875\n",
      "step = 124400: loss = 3164.209228515625\n",
      "step = 124600: loss = 2051.03173828125\n",
      "step = 124800: loss = 6895.8955078125\n",
      "step = 125000: loss = 8776.66015625\n",
      "step = 125000: Average Return = 338.40\n",
      "step = 125200: loss = 7529.25\n",
      "step = 125400: loss = 10954.853515625\n",
      "step = 125600: loss = 6224.0751953125\n",
      "step = 125800: loss = 7696.88134765625\n",
      "step = 126000: loss = 4929.62890625\n",
      "step = 126000: Average Return = 297.60\n",
      "step = 126200: loss = 6452.92431640625\n",
      "step = 126400: loss = 4867.2724609375\n",
      "step = 126600: loss = 3391.57470703125\n",
      "step = 126800: loss = 5016.62158203125\n",
      "step = 127000: loss = 9988.6591796875\n",
      "step = 127000: Average Return = 197.60\n",
      "step = 127200: loss = 6960.224609375\n",
      "step = 127400: loss = 6049.3173828125\n",
      "step = 127600: loss = 1463.6387939453125\n",
      "step = 127800: loss = 3471.5205078125\n",
      "step = 128000: loss = 4197.17041015625\n",
      "step = 128000: Average Return = 310.00\n",
      "step = 128200: loss = 1881.452880859375\n",
      "step = 128400: loss = 7112.57470703125\n",
      "step = 128600: loss = 9873.427734375\n",
      "step = 128800: loss = 2906.18701171875\n",
      "step = 129000: loss = 2553.208251953125\n",
      "step = 129000: Average Return = 367.20\n",
      "step = 129200: loss = 3441.08984375\n",
      "step = 129400: loss = 7793.412109375\n",
      "step = 129600: loss = 5088.40771484375\n",
      "step = 129800: loss = 8008.14453125\n",
      "step = 130000: loss = 8742.9931640625\n",
      "step = 130000: Average Return = 344.40\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 90000\n",
    "for _ in range(num_iterations):\n",
    "  #print(_)\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  for _ in range(collect_steps_per_iteration):\n",
    "    #print(_)\n",
    "    collect_step(train_env, agent.collect_policy)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience)\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1:.2f}'.format(step, avg_return))\n",
    "    returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Step')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = range(0, step + 1, eval_interval)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.environments.tf_py_environment.TFPyEnvironment at 0x2933bee0a60>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step = eval_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_score(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_score = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_score = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_score += time_step._get_info[0]\n",
    "    total_score += episode_score\n",
    "\n",
    "  avg_return = total_score / num_episodes\n",
    "  return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_mp4(filename):\n",
    "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
    "  video = open(filename,'rb').read()\n",
    "  b64 = base64.b64encode(video)\n",
    "  tag = '''\n",
    "  <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "  </video>'''.format(b64.decode())\n",
    "\n",
    "  return IPython.display.HTML(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NeedDownloadError",
     "evalue": "Need ffmpeg exe. You can obtain it with either:\n  - install using conda: conda install ffmpeg -c conda-forge\n  - download using the command: imageio_download_bin ffmpeg\n  - download by calling (in Python): imageio.plugins.ffmpeg.download()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNeedDownloadError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ellio\\work\\MiniProjects\\ML-projects\\2048\\2048_tf.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ellio/work/MiniProjects/ML-projects/2048/2048_tf.ipynb#ch0000020?line=0'>1</a>\u001b[0m num_episodes \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ellio/work/MiniProjects/ML-projects/2048/2048_tf.ipynb#ch0000020?line=1'>2</a>\u001b[0m video_filename \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mimageio.mp4\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ellio/work/MiniProjects/ML-projects/2048/2048_tf.ipynb#ch0000020?line=2'>3</a>\u001b[0m \u001b[39mwith\u001b[39;00m imageio\u001b[39m.\u001b[39;49mget_writer(video_filename, fps\u001b[39m=\u001b[39;49m\u001b[39m60\u001b[39;49m) \u001b[39mas\u001b[39;00m video:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ellio/work/MiniProjects/ML-projects/2048/2048_tf.ipynb#ch0000020?line=3'>4</a>\u001b[0m   \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_episodes):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ellio/work/MiniProjects/ML-projects/2048/2048_tf.ipynb#ch0000020?line=4'>5</a>\u001b[0m     time_step \u001b[39m=\u001b[39m eval_env\u001b[39m.\u001b[39mreset()\n",
      "File \u001b[1;32mc:\\Users\\ellio\\anaconda3\\lib\\site-packages\\imageio\\core\\functions.py:187\u001b[0m, in \u001b[0;36mget_writer\u001b[1;34m(uri, format, mode, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/core/functions.py?line=181'>182</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/core/functions.py?line=182'>183</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not find a format to write the specified file \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39min mode \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m mode\n\u001b[0;32m    <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/core/functions.py?line=183'>184</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/core/functions.py?line=185'>186</a>\u001b[0m \u001b[39m# Return its writer object\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/core/functions.py?line=186'>187</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39;49m\u001b[39m.\u001b[39;49mget_writer(request)\n",
      "File \u001b[1;32mc:\\Users\\ellio\\anaconda3\\lib\\site-packages\\imageio\\core\\format.py:188\u001b[0m, in \u001b[0;36mFormat.get_writer\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/core/format.py?line=183'>184</a>\u001b[0m \u001b[39mif\u001b[39;00m select_mode \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodes:\n\u001b[0;32m    <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/core/format.py?line=184'>185</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/core/format.py?line=185'>186</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFormat \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m cannot write in mode \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname, select_mode)\n\u001b[0;32m    <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/core/format.py?line=186'>187</a>\u001b[0m     )\n\u001b[1;32m--> <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/core/format.py?line=187'>188</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mWriter(\u001b[39mself\u001b[39;49m, request)\n",
      "File \u001b[1;32mc:\\Users\\ellio\\anaconda3\\lib\\site-packages\\imageio\\core\\format.py:224\u001b[0m, in \u001b[0;36mFormat._BaseReaderWriter.__init__\u001b[1;34m(self, format, request)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/core/format.py?line=221'>222</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request \u001b[39m=\u001b[39m request\n\u001b[0;32m    <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/core/format.py?line=222'>223</a>\u001b[0m \u001b[39m# Open the reader/writer\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/core/format.py?line=223'>224</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest\u001b[39m.\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mcopy())\n",
      "File \u001b[1;32mc:\\Users\\ellio\\anaconda3\\lib\\site-packages\\imageio\\plugins\\ffmpeg.py:676\u001b[0m, in \u001b[0;36mFfmpegFormat.Writer._open\u001b[1;34m(self, fps, codec, bitrate, pixelformat, ffmpeg_params, input_params, output_params, ffmpeg_log_level, quality, macro_block_size)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/plugins/ffmpeg.py?line=662'>663</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open\u001b[39m(\n\u001b[0;32m    <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/plugins/ffmpeg.py?line=663'>664</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/plugins/ffmpeg.py?line=664'>665</a>\u001b[0m     fps\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/plugins/ffmpeg.py?line=673'>674</a>\u001b[0m     macro_block_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m,\n\u001b[0;32m    <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/plugins/ffmpeg.py?line=674'>675</a>\u001b[0m ):\n\u001b[1;32m--> <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/plugins/ffmpeg.py?line=675'>676</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_exe()\n\u001b[0;32m    <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/plugins/ffmpeg.py?line=676'>677</a>\u001b[0m     \u001b[39m# Get local filename\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/plugins/ffmpeg.py?line=677'>678</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_filename \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39mget_local_filename()\n",
      "File \u001b[1;32mc:\\Users\\ellio\\anaconda3\\lib\\site-packages\\imageio\\plugins\\ffmpeg.py:660\u001b[0m, in \u001b[0;36mFfmpegFormat.Writer._get_exe\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/plugins/ffmpeg.py?line=657'>658</a>\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/plugins/ffmpeg.py?line=658'>659</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_exe\u001b[39m(\u001b[39mcls\u001b[39m):\n\u001b[1;32m--> <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/plugins/ffmpeg.py?line=659'>660</a>\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_exe \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_exe \u001b[39mor\u001b[39;00m get_exe()\n\u001b[0;32m    <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/plugins/ffmpeg.py?line=660'>661</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_exe\n",
      "File \u001b[1;32mc:\\Users\\ellio\\anaconda3\\lib\\site-packages\\imageio\\plugins\\ffmpeg.py:126\u001b[0m, in \u001b[0;36mget_exe\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/plugins/ffmpeg.py?line=122'>123</a>\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/plugins/ffmpeg.py?line=124'>125</a>\u001b[0m \u001b[39m# Nothing was found so far\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/plugins/ffmpeg.py?line=125'>126</a>\u001b[0m \u001b[39mraise\u001b[39;00m NeedDownloadError(\n\u001b[0;32m    <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/plugins/ffmpeg.py?line=126'>127</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mNeed ffmpeg exe. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/plugins/ffmpeg.py?line=127'>128</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mYou can obtain it with either:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/plugins/ffmpeg.py?line=128'>129</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m  - install using conda: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/plugins/ffmpeg.py?line=129'>130</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mconda install ffmpeg -c conda-forge\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/plugins/ffmpeg.py?line=130'>131</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m  - download using the command: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/plugins/ffmpeg.py?line=131'>132</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimageio_download_bin ffmpeg\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/plugins/ffmpeg.py?line=132'>133</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m  - download by calling (in Python): \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/plugins/ffmpeg.py?line=133'>134</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimageio.plugins.ffmpeg.download()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/ellio/anaconda3/lib/site-packages/imageio/plugins/ffmpeg.py?line=134'>135</a>\u001b[0m )\n",
      "\u001b[1;31mNeedDownloadError\u001b[0m: Need ffmpeg exe. You can obtain it with either:\n  - install using conda: conda install ffmpeg -c conda-forge\n  - download using the command: imageio_download_bin ffmpeg\n  - download by calling (in Python): imageio.plugins.ffmpeg.download()\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 3\n",
    "video_filename = 'imageio.mp4'\n",
    "with imageio.get_writer(video_filename, fps=60) as video:\n",
    "  for _ in range(num_episodes):\n",
    "    time_step = eval_env.reset()\n",
    "    video.append_data(eval_py_env.render())\n",
    "    while not time_step.is_last():\n",
    "      action_step = agent.policy.action(time_step)\n",
    "      time_step = eval_env.step(action_step.action)\n",
    "      video.append_data(eval_py_env.render())\n",
    "\n",
    "embed_mp4(video_filename)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af8704e19de21468eae1093621a2c8b6a787155c45e4386acc9bc35f30ec814d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
